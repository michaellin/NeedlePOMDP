{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiger Tutorial: Solving POMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial outlines how to define a POMDP using the [POMDPs.jl](https://github.com/JuliaPOMDP/POMDPs.jl) interface. We will go through a simple problem simply known as the tiger problem (we will refer to it as the tiger POMDP). After defining the tiger POMDP, we will use QMDP and SARSOP to solve the POMDP. If you are new to working with this package, check out the [tutorial](http://nbviewer.ipython.org/github/JuliaPOMDP/POMDPs.jl/blob/master/examples/GridWorld.ipynb) on MDPs first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "You need to install a few modules in order to use this notebook. If you have all the modules below installed, great! If not run the following commands:\n",
    "\n",
    "```julia\n",
    "# install the POMDPs.jl interface\n",
    "Pkg.clone(\"https://github.com/sisl/POMDPs.jl.git\")\n",
    "\n",
    "using POMDPs\n",
    "\n",
    "# install the SARSOP wrapper\n",
    "POMDPs.add(\"SARSOP\") # note this downloads and builds the APPL toolit and may take a few minutes \n",
    "\n",
    "# install the QMDP solver\n",
    "POMDPs.add(\"QMDP\")\n",
    "\n",
    "# install a helper modules\n",
    "POMDPs.add(\"POMDPToolbox\") # this provides implementations of discrete belief updating\n",
    "\n",
    "# install a Julia packages for working with distributions\n",
    "Pkg.add(\"Distributions\")\n",
    "```\n",
    "\n",
    "If you already have all of the modules above, make sure you have the most recent versions. Many of these are still under heavy development, so update before starting by running\n",
    "\n",
    "```julia\n",
    "Pkg.update()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Nothing to be done\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Overview\n",
    "In the tiger POMDP, the agent is tasked with escaping from a room. There are two doors leading out of the room. Behind one of the doors is a tiger, and behind the other is sweet, sweet freedom. If the agent opens the door and finds the tiger, it gets eaten (and receives a reward of -100). If the agent opens the other door, it escapes and receives a reward of 10. The agent can also listen. Listening gives a noisy measuremnt of which door the tiger is hiding behind. Listening gives the agent the correct location of the tiger 85% of the time. The agent receives a reward of -1 for listening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first import POMDPs.jl\n",
    "using POMDPs\n",
    "POMDPs.add(\"SARSOP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMDP \n",
    "A POMDP is defined by the tuple\n",
    "$$(\\mathcal{S}, \\mathcal{A}, \\mathcal{Z}, T, R, O).$$\n",
    "In addition to the familiar, state $\\mathcal{S}$ and action $\\mathcal{A}$ spaces, we must also define an observation space $\\mathcal{Z}$ and an observation function $O$. The POMDP problem definition may be similar to the one for MDP. For example, if you wanted to add state uncertaitniy to your problem, you can define the observation space, and observation function in addition to your previous MDP definition.\n",
    "\n",
    "Before defining the spaces for this problem, let's first deinfe the concrete type for the tiger POMDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.TigerPOMDP})(Float64, Float64, Float64, Float64, Float64) in module Main at In[36]:2 overwritten at In[65]:2.\n",
      "WARNING: Method definition (::Type{Main.TigerPOMDP})(Any, Any, Any, Any, Any) in module Main at In[36]:2 overwritten at In[65]:2.\n",
      "WARNING: Method definition (::Type{Main.TigerPOMDP})() in module Main at In[36]:10 overwritten at In[65]:10.\n"
     ]
    }
   ],
   "source": [
    "type TigerPOMDP <: POMDP{Bool, Symbol, Bool} # POMDP{State, Action, Observation} all parametarized by Int64s\n",
    "    r_listen::Float64 # reward for listening (default -1)\n",
    "    r_findtiger::Float64 # reward for finding the tiger (default -100)\n",
    "    r_escapetiger::Float64 # reward for escaping (default 10)\n",
    "    p_listen_correctly::Float64 # prob of correctly listening (default 0.85)\n",
    "    discount_factor::Float64 # discount\n",
    "end\n",
    "# default constructor\n",
    "function TigerPOMDP()\n",
    "    return TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters in the problem definition, but we can treat them all as constants. You can read more about the Tiger problem and POMDPs [here](http://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_tutorial.pdf#page=28). However, we created a default constructor that allows us to initialize the tiger POMDP by simply running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerPOMDP(-1.0,-100.0,10.0,0.85,0.95)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp = TigerPOMDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the TigerPOMDP type inherits from a ```POMDP{Bool, Symbol, Bool}```. This means that in our problem we will use ```Bool``` to represent our states, ```Symbol``` to represent our actions and ```Bool``` to represent our observations. More details on states, actions and observations in this problem are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States\n",
    "We define our state with a boolean that indicates weather or not the tiger is hiding behind the left door. If our state is ```true```, the tiger is behind the left door. If its ```false```, the tiger is behind the right door. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition create_state(Main.TigerPOMDP) in module Main at In[38]:3 overwritten at In[67]:3.\n"
     ]
    }
   ],
   "source": [
    "example_state = false # tiger is hiding behind right door\n",
    "# initialization function\n",
    "POMDPs.create_state(::TigerPOMDP) = true;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the state is a binary value, we represent it as a boolean, but we could have represented it as an integer or any other sensible type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "There are three possible actions our agent can take: open the left door, open the right door, and listen. For clarity, we will represent these with symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition create_action(Main.TigerPOMDP) in module Main at In[39]:3 overwritten at In[68]:3.\n"
     ]
    }
   ],
   "source": [
    "example_action = :listen # agent listens, can be :openl or :openr\n",
    "# initialization function\n",
    "POMDPs.create_action(::TigerPOMDP) = :listen;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent our actions with the following symbols: open left (:openl), open right (:openr), and listen (:listen). For example, the action below represnts listening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":listen"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = :listen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "There are two possible observations: the agent either hears the tiger behind the left door or behind the right door. We use a boolean to represent the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition create_observation(Main.TigerPOMDP) in module Main at In[41]:3 overwritten at In[70]:3.\n"
     ]
    }
   ],
   "source": [
    "example_observation = true # agent heard the tiger behind the left door\n",
    "# initialization function\n",
    "POMDPs.create_observation(::TigerPOMDP) = true;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces\n",
    "Let's define our state, action and observation spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Space\n",
    "There are only two states in the tiger POMDP: the tiger is either behind the left door or behind the right door. Our state space is simply an array of the states in the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.TigerStateSpace})(Array{Bool, 1}) in module Main at In[42]:2 overwritten at In[71]:2.\n",
      "WARNING: Method definition (::Type{Main.TigerStateSpace})(Any) in module Main at In[42]:2 overwritten at In[71]:2.\n"
     ]
    }
   ],
   "source": [
    "type TigerStateSpace <: AbstractSpace\n",
    "    states::Vector{Bool} \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the ```states``` and the ```iterator``` functions. Recall, that the ```states``` function returns the state space for a given POMDP type, and the ```iterator``` function returns an iterator for a given space. Here, the iterator function returns an array of the two possible states in our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition states(Main.TigerPOMDP) in module Main at In[43]:1 overwritten at In[72]:1.\n",
      "WARNING: Method definition iterator(Main.TigerStateSpace) in module Main at In[43]:2 overwritten at In[72]:2.\n",
      "WARNING: Method definition state_index(Main.TigerPOMDP, Bool) in module Main at In[43]:3 overwritten at In[72]:3.\n"
     ]
    }
   ],
   "source": [
    "POMDPs.states(::TigerPOMDP) = TigerStateSpace([true, false])\n",
    "POMDPs.iterator(space::TigerStateSpace) = space.states;\n",
    "POMDPs.state_index(::TigerPOMDP, s::Bool) = (Int64(s) + 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the [tutorial](http://nbviewer.ipython.org/github/sisl/POMDPs.jl/blob/master/examples/GridWorld.ipynb) on MDPs, we also defined a ```rand!``` function that sampled the space. We do not need this function when using QMDP or SARSOP. However, if you wanted to use Monte Carlo solvers solvers like POMCP or DESPOT you would need a function that can sample your spaces. It may be convenient to have these sampling functions if you plan to simulate your policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "There are three actions in our problem. Once again, we represent the action space as an array of the actions in our problem. The ```actions``` and ```iterator``` functions serve a similar purpose to the ```states``` and ```iterator``` functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.TigerActionSpace})(Array{Symbol, 1}) in module Main at In[44]:2 overwritten at In[73]:2.\n",
      "WARNING: Method definition (::Type{Main.TigerActionSpace})(Any) in module Main at In[44]:2 overwritten at In[73]:2.\n",
      "WARNING: Method definition actions(Main.TigerPOMDP) in module Main at In[44]:5 overwritten at In[73]:5.\n",
      "WARNING: Method definition actions(Main.TigerPOMDP, Bool, Main.TigerActionSpace) in module Main at In[44]:6 overwritten at In[73]:6.\n",
      "WARNING: Method definition iterator(Main.TigerActionSpace) in module Main at In[44]:8 overwritten at In[73]:8.\n"
     ]
    }
   ],
   "source": [
    "type TigerActionSpace <: AbstractSpace\n",
    "    actions::Vector{Symbol} \n",
    "end\n",
    "# define actions function\n",
    "POMDPs.actions(::TigerPOMDP) = TigerActionSpace([:openl, :openr, :listen]); # default\n",
    "POMDPs.actions(::TigerPOMDP, state::Bool, acts::TigerActionSpace) = acts; # convenience (actions do not change in different states)\n",
    "# define iterator function\n",
    "POMDPs.iterator(space::TigerActionSpace) = space.actions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space\n",
    "The observation space looks similar to the state space. Recall that the state represents the truth about our system, while the observation is potentially false information recieves about the state. In the tiger POMDP, our observation could give us a false representation of our state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.TigerObservationSpace})(Array{Bool, 1}) in module Main at In[45]:2 overwritten at In[74]:2.\n",
      "WARNING: Method definition (::Type{Main.TigerObservationSpace})(Any) in module Main at In[45]:2 overwritten at In[74]:2.\n",
      "WARNING: Method definition observations(Main.TigerPOMDP) in module Main at In[45]:5 overwritten at In[74]:5.\n",
      "WARNING: Method definition observations(Main.TigerPOMDP, Bool, Main.TigerObservationSpace) in module Main at In[45]:6 overwritten at In[74]:6.\n",
      "WARNING: Method definition iterator(Main.TigerObservationSpace) in module Main at In[45]:8 overwritten at In[74]:8.\n"
     ]
    }
   ],
   "source": [
    "type TigerObservationSpace <: AbstractSpace\n",
    "    obs::Vector{Bool} \n",
    "end\n",
    "# function returning observation space\n",
    "POMDPs.observations(::TigerPOMDP) = TigerObservationSpace([true, false]);\n",
    "POMDPs.observations(::TigerPOMDP, s::Bool, obs::TigerObservationSpace) = obs;\n",
    "# function returning an iterator over that space\n",
    "POMDPs.iterator(space::TigerObservationSpace) = space.obs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the POMDP spaces, let's move on to defining the model functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition and Observation Distributions\n",
    "Before defining the model functions, we first need to create a distributions data-type. In general, our distributions should support sampling and have a ```pdf``` method. If you only want to get a policy from the SARSOP and QMDP solvers, you do not need to worry about implementing a sampling function. However, if you want to simulate the policy, you should implement these functions.\n",
    "\n",
    "Since the transition and observation distributions have identical form, we could just use a single type to serve the needs of both. This will not be the case in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition (::Type{Main.TigerDistribution})(Float64, Array{Bool, 1}) in module Main at In[46]:3 overwritten at In[75]:3.\n",
      "WARNING: Method definition (::Type{Main.TigerDistribution})(Any, Any) in module Main at In[46]:3 overwritten at In[75]:3.\n",
      "WARNING: Method definition (::Type{Main.TigerDistribution})() in module Main at In[46]:6 overwritten at In[75]:6.\n",
      "WARNING: Method definition iterator(Main.TigerDistribution) in module Main at In[46]:7 overwritten at In[75]:7.\n",
      "WARNING: Method definition create_transition_distribution(Main.TigerPOMDP) in module Main at In[46]:9 overwritten at In[75]:9.\n",
      "WARNING: Method definition create_observation_distribution(Main.TigerPOMDP) in module Main at In[46]:10 overwritten at In[75]:10.\n"
     ]
    }
   ],
   "source": [
    "# distribution type that will be used for both transitions and observations\n",
    "type TigerDistribution <: AbstractDistribution\n",
    "    p::Float64\n",
    "    it::Vector{Bool}\n",
    "end\n",
    "TigerDistribution() = TigerDistribution(0.5, [true, false])\n",
    "POMDPs.iterator(d::TigerDistribution) = d.it\n",
    "\n",
    "POMDPs.create_transition_distribution(::TigerPOMDP) = TigerDistribution()\n",
    "POMDPs.create_observation_distribution(::TigerPOMDP) = TigerDistribution();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the ```pdf``` function. For a discrete problem, this function returns the probability of a given element (state or observation in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition pdf(Main.TigerDistribution, Bool) in module Main at In[47]:3 overwritten at In[76]:3.\n"
     ]
    }
   ],
   "source": [
    "# transition and observation pdf\n",
    "function POMDPs.pdf(d::TigerDistribution, so::Bool)\n",
    "    so ? (return d.p) : (return 1.0-d.p)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the sampling functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition rand(Base.Random.AbstractRNG, Main.TigerDistribution, Bool) in module Main at In[48]:2 overwritten at In[77]:2.\n"
     ]
    }
   ],
   "source": [
    "# samples from transition or observation distribution\n",
    "POMDPs.rand(rng::AbstractRNG, d::TigerDistribution, so::Bool) = rand(rng) <= d.p;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we have to do for our distribution functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Model\n",
    "Here we define the transition model for the tiger POMDP. The model itself is fairly simple. Our state is represented by the location of the tiger (left or right). The location of the tiger doesn't change when the agent listens. However, after the agent opens the door, it reaches a terminal state. That is the agent either escapes or gets eaten. To simplify our formulation, we simply reset the location of the tiger randomly. We could model this problem with a terminal state (i.e. one in which the agent no longer receives reward) as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition transition(Main.TigerPOMDP, Bool, Symbol) in module Main at In[49]:3 overwritten at In[78]:3.\n",
      "WARNING: Method definition transition(Main.TigerPOMDP, Bool, Symbol, Main.TigerDistribution) in module Main at In[49]:3 overwritten at In[78]:3.\n"
     ]
    }
   ],
   "source": [
    "# Resets the problem after opening door; does nothing after listening\n",
    "function POMDPs.transition(pomdp::TigerPOMDP, s::Bool, a::Symbol, d::TigerDistribution=create_transition_distribution(pomdp))\n",
    "    if a == :openl || a == :openr\n",
    "        d.p = 0.5\n",
    "    elseif s\n",
    "        d.p = 1.0\n",
    "    else\n",
    "        d.p = 0.0\n",
    "    end\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Model\n",
    "The reward model caputres the immediate objectives of the agent. It recieves a large negative reward for opening the door with the tiger behind it (-100), gets a positive reward for opening the other door (+10), and a small penalty for listening (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition reward(Main.TigerPOMDP, Bool, Symbol) in module Main at In[50]:3 overwritten at In[79]:3.\n",
      "WARNING: Method definition reward(Main.TigerPOMDP, Bool, Symbol, Bool) in module Main at In[50]:13 overwritten at In[79]:13.\n"
     ]
    }
   ],
   "source": [
    "# reward model\n",
    "function POMDPs.reward(pomdp::TigerPOMDP, s::Bool, a::Symbol)\n",
    "    r = 0.0\n",
    "    a == :listen ? (r+=pomdp.r_listen) : (nothing)\n",
    "    if a == :openl\n",
    "        s ? (r += pomdp.r_findtiger) : (r += pomdp.r_escapetiger)\n",
    "    end\n",
    "    if a == :openr\n",
    "        s ? (r += pomdp.r_escapetiger) : (r += pomdp.r_findtiger)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "POMDPs.reward(pomdp::TigerPOMDP, s::Bool, a::Symbol, sp::Bool) = reward(pomdp, s, a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Model\n",
    "The observation model captures the uncertaintiy in the agent's lsitening ability. When we listen, we receive a noisy measurement of the tiger's location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition observation(Main.TigerPOMDP, Symbol, Bool) in module Main at In[51]:3 overwritten at In[80]:3.\n",
      "WARNING: Method definition observation(Main.TigerPOMDP, Symbol, Bool, Main.TigerDistribution) in module Main at In[51]:3 overwritten at In[80]:3.\n",
      "WARNING: Method definition observation(Main.TigerPOMDP, Bool, Symbol, Bool) in module Main at In[51]:11 overwritten at In[80]:11.\n",
      "WARNING: Method definition observation(Main.TigerPOMDP, Bool, Symbol, Bool, Main.TigerDistribution) in module Main at In[51]:11 overwritten at In[80]:11.\n"
     ]
    }
   ],
   "source": [
    "# observation model\n",
    "function POMDPs.observation(pomdp::TigerPOMDP, a::Symbol, s::Bool, d::TigerDistribution=create_observation_distribution(pomdp))\n",
    "    pc = pomdp.p_listen_correctly\n",
    "    if a == :listen\n",
    "        s ? (d.p = pc) : (d.p = 1.0-pc)\n",
    "    else\n",
    "        d.p = 0.5\n",
    "    end\n",
    "    d\n",
    "end\n",
    "POMDPs.observation(pomdp::TigerPOMDP, \n",
    "                   s::Bool, \n",
    "                   a::Symbol, \n",
    "                   sp::Bool, \n",
    "                   d::TigerDistribution=create_observation_distribution(pomdp)) = observation(pomdp, a, sp, d);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscallenous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the ```discount``` function and the functions that return the size of our spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition discount(Main.TigerPOMDP) in module Main at In[62]:1 overwritten at In[81]:1.\n",
      "WARNING: Method definition n_states(Main.TigerPOMDP) in module Main at In[62]:2 overwritten at In[81]:2.\n",
      "WARNING: Method definition n_actions(Main.TigerPOMDP) in module Main at In[62]:3 overwritten at In[81]:3.\n",
      "WARNING: Method definition n_observations(Main.TigerPOMDP) in module Main at In[62]:4 overwritten at In[81]:4.\n"
     ]
    }
   ],
   "source": [
    "POMDPs.discount(pomdp::TigerPOMDP) = pomdp.discount_factor\n",
    "POMDPs.n_states(::TigerPOMDP) = 2\n",
    "POMDPs.n_actions(::TigerPOMDP) = 3\n",
    "POMDPs.n_observations(::TigerPOMDP) = 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beliefs\n",
    "If you are somewhat familiar with Julia defining all of the above may have been relaitvely simple. However, all POMDPs must be represented with a belief. Implementing beliefs and their updaters can be tricky. Luckily, our solvers abstract away the belief updating. All you need to do is define a function that returns an initial distriubtion over states. This distribution needs to support ```pdf``` and ```rand``` function. We already defined a dsitribution like that, so our job here is simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition initial_state_distribution(Main.TigerPOMDP) in module Main at In[63]:1 overwritten at In[82]:1.\n"
     ]
    }
   ],
   "source": [
    "POMDPs.initial_state_distribution(pomdp::TigerPOMDP) = TigerDistribution(0.5, [true, false]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in creating your own beliefs and update schemes check out the [POMDPToolbox](https://github.com/JuliaPOMDP/POMDPToolbox.jl) module which implements a number of beliefs and udpate schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSOP Solver\n",
    "Let's play around with the [SARSOP.jl](https://github.com/sisl/SARSOP.jl) solver. The module we provide is a wrapper for the SARSOP backend. You can find more information about it [here](http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a pomdpx file: model.pomdpx\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: POMDPs.jl: No implementation of initial_state_distribution for pomdp::TigerPOMDP.\nwhile loading In[83], in expression starting on line 8",
     "output_type": "error",
     "traceback": [
      "LoadError: POMDPs.jl: No implementation of initial_state_distribution for pomdp::TigerPOMDP.\nwhile loading In[83], in expression starting on line 8",
      "",
      " in initial_state_distribution(::TigerPOMDP) at ./<missing>:0",
      " in belief_xml(::TigerPOMDP, ::POMDPXFiles.POMDPXFile, ::IOStream) at /Users/michaellin/.julia/v0.5/POMDPXFiles/src/writer.jl:263",
      " in write(::TigerPOMDP, ::POMDPXFiles.POMDPXFile) at /Users/michaellin/.julia/v0.5/POMDPXFiles/src/writer.jl:126",
      " in SARSOP.POMDPFile(::TigerPOMDP, ::String) at /Users/michaellin/.julia/v0.5/SARSOP/src/file.jl:14",
      " in solve(::SARSOP.SARSOPSolver, ::TigerPOMDP, ::SARSOP.POMDPPolicy) at /Users/michaellin/.julia/v0.5/SARSOP/src/solver.jl:98 (repeats 2 times)"
     ]
    }
   ],
   "source": [
    "using SARSOP # load the module\n",
    "# initialize our tiger POMDP\n",
    "pomdp = TigerPOMDP()\n",
    "\n",
    "# initialize the solver\n",
    "solver = SARSOPSolver()\n",
    "# run the solve function\n",
    "policy = solve(solver, pomdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: policy not defined\nwhile loading In[84], in expression starting on line 2",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: policy not defined\nwhile loading In[84], in expression starting on line 2",
      ""
     ]
    }
   ],
   "source": [
    "# we can retrieve the alpha vectors by calling\n",
    "alphas(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how our policy changes with the belief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POMDPToolbox.DiscreteBelief([0.5,0.5],[0.5,0.5],2,true)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use POMDPToolbox for beliefs\n",
    "using POMDPToolbox\n",
    "\n",
    "# let's initialize the beliefs\n",
    "b = DiscreteBelief(2); # the initial prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: policy not defined\nwhile loading In[86], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: policy not defined\nwhile loading In[86], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "a = action(policy, b) # index of action, you need to convert this to the true action, support for automatic conversion is coming soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate our policy. We'll use POMDPToolbox to do the simulation. As mentioned earlier, in a POMDP, the decision is based on a belief. However, each policy (comes from the solver modules) implements its own belief udpating scheme, so you do not need to worry about deling with beliefs. The only thing you need is to define an ```initial_state_distribution```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: policy not defined\nwhile loading In[87], in expression starting on line 6",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: policy not defined\nwhile loading In[87], in expression starting on line 6",
      ""
     ]
    }
   ],
   "source": [
    "using POMDPToolbox # for simulation\n",
    "\n",
    "pomdp = TigerPOMDP() # initialize problem\n",
    "init_dist = initial_state_distribution(pomdp) # initialize distriubtion over state\n",
    "\n",
    "up = updater(policy) # belief updater for our policy, SARSOP uses a discrete Bayesian filter\n",
    "hist = HistoryRecorder(max_steps=100, rng=MersenneTwister(1)) # history recorder that keeps track of states, observations and beliefs\n",
    "\n",
    "r = simulate(hist, pomdp, policy, up, init_dist)\n",
    "\n",
    "println(\"Total reward: $r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that over the first six time steps, the policy is fairly simple. We listen twice, and then decide which door to open. However, in the following steps, we get a mix of observations, which makes the decision harder. Our agent does not open a door, because its belief is still uniform at the last time step! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QMDP Solver\n",
    "We will briefly go over the [QMDP.jl](https://github.com/sisl/QMDP.jl) solver. You should use QMDP with a word of caution. QMDP assumes that all state uncetainty dissapears in the next time step. This could lead to bad policies in problems with information gathering actions. For example, in the tiger POMDP listening is an information gathering action, and the resulting QMDP policy is quite poor. However, QMDP can work very well in problems where the state uncertainity is not impacted by the agent's action (for example systems with noisy sensor measurements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1, residual: 14.75, iteration run-time: 1.6844e-5, total run-time: 1.6844e-5\n",
      "Iteration : 2, residual: 12.59046875, iteration run-time: 1.1602e-5, total run-time: 2.8446e-5\n",
      "Iteration : 3, residual: 11.564691406249999, iteration run-time: 9.16e-6, total run-time: 3.7606e-5\n",
      "Iteration : 4, residual: 10.943236428222654, iteration run-time: 3.057e-6, total run-time: 4.0663e-5\n",
      "Iteration : 5, residual: 10.2558588273941, iteration run-time: 2.781e-6, total run-time: 4.3444e-5\n",
      "Iteration : 6, residual: 9.587976314837448, iteration run-time: 2.569e-6, total run-time: 4.6013e-5\n",
      "Iteration : 7, residual: 8.957886507199987, iteration run-time: 2.211e-6, total run-time: 4.8224000000000004e-5\n",
      "Iteration : 8, residual: 8.367828168991792, iteration run-time: 2.195e-6, total run-time: 5.0419000000000006e-5\n",
      "Iteration : 9, residual: 7.816304847983972, iteration run-time: 2.441e-6, total run-time: 5.2860000000000006e-5\n",
      "Iteration : 10, residual: 7.301052156282381, iteration run-time: 2.214e-6, total run-time: 5.507400000000001e-5\n",
      "Iteration : 11, residual: 6.8197456599030915, iteration run-time: 2.555e-6, total run-time: 5.762900000000001e-5\n",
      "Iteration : 12, residual: 6.370163598662359, iteration run-time: 2.774e-6, total run-time: 6.0403000000000004e-5\n",
      "Iteration : 13, residual: 5.9502184661618, iteration run-time: 2.498e-6, total run-time: 6.2901e-5\n",
      "Iteration : 14, residual: 5.557957422333274, iteration run-time: 2.226e-6, total run-time: 6.5127e-5\n",
      "Iteration : 15, residual: 5.191555653202798, iteration run-time: 2.06e-6, total run-time: 6.7187e-5\n",
      "Iteration : 16, residual: 4.849308471382614, iteration run-time: 2.021e-6, total run-time: 6.9208e-5\n",
      "Iteration : 17, residual: 4.529623527415282, iteration run-time: 2.057e-6, total run-time: 7.1265e-5\n",
      "Iteration : 18, residual: 4.231013435561891, iteration run-time: 2.302e-6, total run-time: 7.356700000000001e-5\n",
      "Iteration : 19, residual: 3.9520888618093863, iteration run-time: 2.165e-6, total run-time: 7.573200000000001e-5\n",
      "Iteration : 20, residual: 3.6915520617660036, iteration run-time: 2.147e-6, total run-time: 7.787900000000002e-5\n",
      "Iteration : 21, residual: 3.448190843167879, iteration run-time: 2.387e-6, total run-time: 8.026600000000002e-5\n",
      "Iteration : 22, residual: 3.2208729260632936, iteration run-time: 2.205e-6, total run-time: 8.247100000000002e-5\n",
      "Iteration : 23, residual: 3.00854067471343, iteration run-time: 2.553e-6, total run-time: 8.502400000000002e-5\n",
      "Iteration : 24, residual: 2.8102061767669397, iteration run-time: 2.269e-6, total run-time: 8.729300000000002e-5\n",
      "Iteration : 25, residual: 2.624946646829443, iteration run-time: 2.028e-6, total run-time: 8.932100000000002e-5\n",
      "Iteration : 26, residual: 2.451900133045797, iteration run-time: 2.343e-6, total run-time: 9.166400000000002e-5\n",
      "Iteration : 27, residual: 2.290261506721066, iteration run-time: 1.92e-6, total run-time: 9.358400000000002e-5\n",
      "Iteration : 28, residual: 2.13927871632049, iteration run-time: 2.001e-6, total run-time: 9.558500000000001e-5\n",
      "Iteration : 29, residual: 1.9982492884203396, iteration run-time: 2.056e-6, total run-time: 9.764100000000001e-5\n",
      "Iteration : 30, residual: 1.866517059329368, iteration run-time: 2.333e-6, total run-time: 9.997400000000002e-5\n",
      "Iteration : 31, residual: 1.7434691221742469, iteration run-time: 2.146e-6, total run-time: 0.00010212000000000002\n",
      "Iteration : 32, residual: 1.628532975244866, iteration run-time: 2.269e-6, total run-time: 0.00010438900000000002\n",
      "Iteration : 33, residual: 1.5211738583317356, iteration run-time: 2.147e-6, total run-time: 0.00010653600000000002\n",
      "Iteration : 34, residual: 1.4208922646616031, iteration run-time: 2.045e-6, total run-time: 0.00010858100000000002\n",
      "Iteration : 35, residual: 1.32722161685669, iteration run-time: 2.064e-6, total run-time: 0.00011064500000000002\n",
      "Iteration : 36, residual: 1.2397260961028849, iteration run-time: 2.04e-6, total run-time: 0.00011268500000000002\n",
      "Iteration : 37, residual: 1.1579986144276404, iteration run-time: 2.107e-6, total run-time: 0.00011479200000000002\n",
      "Iteration : 38, residual: 1.0816589206533251, iteration run-time: 2.111e-6, total run-time: 0.00011690300000000002\n",
      "Iteration : 39, residual: 1.0103518312128017, iteration run-time: 2.601e-6, total run-time: 0.00011950400000000001\n",
      "Iteration : 40, residual: 0.9437455775971557, iteration run-time: 2.096e-6, total run-time: 0.00012160000000000001\n",
      "Iteration : 41, residual: 0.8815302627452866, iteration run-time: 1.996e-6, total run-time: 0.00012359600000000001\n",
      "Iteration : 42, residual: 0.8234164191945297, iteration run-time: 2.051e-6, total run-time: 0.00012564700000000003\n",
      "Iteration : 43, residual: 0.7691336622836786, iteration run-time: 1.981e-6, total run-time: 0.00012762800000000002\n",
      "Iteration : 44, residual: 0.7184294321414484, iteration run-time: 2.216e-6, total run-time: 0.00012984400000000003\n",
      "Iteration : 45, residual: 0.6710678186085772, iteration run-time: 2.078e-6, total run-time: 0.00013192200000000003\n",
      "Iteration : 46, residual: 0.6268284636247756, iteration run-time: 2.341e-6, total run-time: 0.00013426300000000003\n",
      "Iteration : 47, residual: 0.5855055359753294, iteration run-time: 2.202e-6, total run-time: 0.00013646500000000003\n",
      "Iteration : 48, residual: 0.5469067736256363, iteration run-time: 2.155e-6, total run-time: 0.00013862000000000002\n",
      "Iteration : 49, residual: 0.5108525891891986, iteration run-time: 2.434e-6, total run-time: 0.00014105400000000002\n",
      "Iteration : 50, residual: 0.4771752343663138, iteration run-time: 1.913e-6, total run-time: 0.00014296700000000003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QMDP.QMDPPolicy([83.2389 193.239 182.354; 193.466 83.6846 182.577],Any[:openl,:openr,:listen],TigerPOMDP(-1.0,-100.0,10.0,0.85,0.95))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using QMDP\n",
    "\n",
    "# initialize the solver\n",
    "# key-word args are the maximum number of iterations the solver will run for, and the Bellman tolerance\n",
    "solver = QMDPSolver(max_iterations=50, tolerance=1e-3) \n",
    "\n",
    "# run the solver\n",
    "qmdp_policy = solve(solver, pomdp, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2x3 Array{Float64,2}:\n",
       "  83.2389  193.239   182.354\n",
       " 193.466    83.6846  182.577"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmdp_policy.alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these alpha-vectors differ from those compute by SARSOP. Let's see how the policy looks in simulation. We'll use the same procedure to simulate the QMDP policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 28.951808801120205\n"
     ]
    }
   ],
   "source": [
    "pomdp = TigerPOMDP() # initialize problem\n",
    "init_dist = initial_state_distribution(pomdp) # initialize distriubtion over state\n",
    "\n",
    "up = updater(policy) # belief updater for our policy, SARSOP uses a discrete Bayesian filter\n",
    "hist = HistoryRecorder(max_steps=100, rng=MersenneTwister(1)) # history recorder that keeps track of states, observations and beliefs\n",
    "\n",
    "r = simulate(hist, pomdp, policy, up, init_dist)\n",
    "\n",
    "println(\"Total reward: $r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are identical! At least for this problem. In general, if you are dealing with a complex problem, it is good to compare the SARSOP and QMDP policies. This framework makes comparing the two policies very simple once you have defined the problem! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
